{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install wordcloud\n",
    "%pip install geopy\n",
    "%pip install folium\n",
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn nltk scikit-learn wordcloud geopy folium textblob spacy\n",
    "%python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries for data handling and analysis\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from textblob import TextBlob\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "# load trump tweets data into a pandas dataframe\n",
    "df_trump = pd.read_csv('data/hashtag_donaldtrump.csv', engine='python', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# load biden tweets data into a pandas dataframe\n",
    "df_biden = pd.read_csv('data/hashtag_joebiden.csv', engine='python', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# check the first few rows of trump data to understand its structure\n",
    "print(\"Trump tweets data preview:\")\n",
    "print(df_trump.head())\n",
    "\n",
    "# check the first few rows of biden data to understand its structure\n",
    "print(\"\\nBiden tweets data preview:\")\n",
    "print(df_biden.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns that won't be used in analysis\n",
    "columns_to_drop = ['user_id', 'tweet_id', 'lat', 'long', 'source']\n",
    "df_trump = df_trump.drop(columns=columns_to_drop, errors='ignore')  # drop columns from trump data\n",
    "df_biden = df_biden.drop(columns=columns_to_drop, errors='ignore')  # drop columns from biden data\n",
    "\n",
    "# convert created_at and collected_at columns to datetime format for both datasets\n",
    "df_trump['created_at'] = pd.to_datetime(df_trump['created_at'], errors='coerce')  # trump tweet timestamp\n",
    "df_trump['collected_at'] = pd.to_datetime(df_trump['collected_at'], errors='coerce')  # trump data collection timestamp\n",
    "df_biden['created_at'] = pd.to_datetime(df_biden['created_at'], errors='coerce')  # biden tweet timestamp\n",
    "df_biden['collected_at'] = pd.to_datetime(df_biden['collected_at'], errors='coerce')  # biden data collection timestamp\n",
    "\n",
    "# drop rows where the tweet content is missing\n",
    "df_trump = df_trump.dropna(subset=['tweet'])  # drop missing tweets in trump data\n",
    "df_biden = df_biden.dropna(subset=['tweet'])  # drop missing tweets in biden data\n",
    "\n",
    "# drop rows where the state is missing\n",
    "df_trump = df_trump.dropna(subset=['state'])  # drop missing states in trump data\n",
    "df_biden = df_biden.dropna(subset=['state'])  # drop missing states in biden data\n",
    "\n",
    "# function to clean tweet text by removing URLs, special characters, and standardizing case\n",
    "def clean_tweet(text):  # define function for tweet text cleaning\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # remove urls\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'#', '', text)  # remove hashtag symbol\n",
    "    text = re.sub(r'\\n', ' ', text)  # replace newline chars w spaces\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)  # remove special chars\n",
    "    return text.lower()  # convert text to lowercase\n",
    "\n",
    "# apply the clean_tweet function to trump tweets\n",
    "df_trump['cleaned_tweet'] = df_trump['tweet'].apply(clean_tweet)  # cleaned trump tweets\n",
    "\n",
    "# apply the clean_tweet function to biden tweets\n",
    "df_biden['cleaned_tweet'] = df_biden['tweet'].apply(clean_tweet)  # cleaned biden tweets\n",
    "\n",
    "# text lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# classify sentiment\n",
    "def classify_sentiment(score):\n",
    "    if score > 0.1:\n",
    "        return \"positive\"\n",
    "    elif score < -0.1:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Filter tweets between August and November\n",
    "start_date = '2020-08-01'\n",
    "end_date = '2020-11-30'\n",
    "df_trump['created_at'] = pd.to_datetime(df_trump['created_at'], errors='coerce')\n",
    "df_biden['created_at'] = pd.to_datetime(df_biden['created_at'], errors='coerce')\n",
    "df_trump = df_trump[(df_trump['created_at'] >= start_date) & (df_trump['created_at'] <= end_date)]\n",
    "df_biden = df_biden[(df_biden['created_at'] >= start_date) & (df_biden['created_at'] <= end_date)]\n",
    "\n",
    "# Clean and lemmatize tweets\n",
    "df_trump['cleaned_tweet'] = df_trump['tweet'].apply(clean_tweet).apply(lemmatize_text)\n",
    "df_biden['cleaned_tweet'] = df_biden['tweet'].apply(clean_tweet).apply(lemmatize_text)\n",
    "\n",
    "# Sentiment analysis\n",
    "df_trump['sentiment_score'] = df_trump['cleaned_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df_biden['sentiment_score'] = df_biden['cleaned_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df_trump['sentiment'] = df_trump['sentiment_score'].apply(classify_sentiment)\n",
    "df_biden['sentiment'] = df_biden['sentiment_score'].apply(classify_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trump data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 971073 entries, 0 to 971086\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count   Dtype         \n",
      "---  ------                --------------   -----         \n",
      " 0   created_at            970919 non-null  datetime64[ns]\n",
      " 1   tweet                 971073 non-null  object        \n",
      " 2   likes                 971045 non-null  object        \n",
      " 3   retweet_count         970933 non-null  float64       \n",
      " 4   user_name             970911 non-null  object        \n",
      " 5   user_screen_name      970933 non-null  object        \n",
      " 6   user_description      869661 non-null  object        \n",
      " 7   user_join_date        970779 non-null  object        \n",
      " 8   user_followers_count  970917 non-null  object        \n",
      " 9   user_location         675830 non-null  object        \n",
      " 10  city                  227180 non-null  object        \n",
      " 11  country               442732 non-null  object        \n",
      " 12  continent             442749 non-null  object        \n",
      " 13  state                 320614 non-null  object        \n",
      " 14  state_code            300414 non-null  object        \n",
      " 15  collected_at          2 non-null       datetime64[ns]\n",
      " 16  cleaned_tweet         971073 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(1), object(14)\n",
      "memory usage: 133.4+ MB\n",
      "None\n",
      "\n",
      "Biden data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 776995 entries, 0 to 777072\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count   Dtype         \n",
      "---  ------                --------------   -----         \n",
      " 0   created_at            776886 non-null  datetime64[ns]\n",
      " 1   tweet                 776995 non-null  object        \n",
      " 2   likes                 776914 non-null  object        \n",
      " 3   retweet_count         776895 non-null  float64       \n",
      " 4   user_name             776870 non-null  object        \n",
      " 5   user_screen_name      776895 non-null  object        \n",
      " 6   user_description      694882 non-null  object        \n",
      " 7   user_join_date        776784 non-null  object        \n",
      " 8   user_followers_count  776885 non-null  object        \n",
      " 9   user_location         543063 non-null  object        \n",
      " 10  city                  186869 non-null  object        \n",
      " 11  country               353770 non-null  object        \n",
      " 12  continent             353788 non-null  object        \n",
      " 13  state                 260191 non-null  object        \n",
      " 14  state_code            244603 non-null  object        \n",
      " 15  collected_at          5 non-null       datetime64[ns]\n",
      " 16  cleaned_tweet         776995 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(1), object(14)\n",
      "memory usage: 106.7+ MB\n",
      "None\n",
      "\n",
      "Trump tweet length statistics:\n",
      "count    971073.000000\n",
      "mean        139.393067\n",
      "std          77.491990\n",
      "min           2.000000\n",
      "25%          73.000000\n",
      "50%         130.000000\n",
      "75%         210.000000\n",
      "max         331.000000\n",
      "Name: tweet_length, dtype: float64\n",
      "\n",
      "Biden tweet length statistics:\n",
      "count    776995.000000\n",
      "mean        133.127283\n",
      "std          76.727705\n",
      "min           1.000000\n",
      "25%          68.000000\n",
      "50%         121.000000\n",
      "75%         200.000000\n",
      "max         323.000000\n",
      "Name: tweet_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# eda\n",
    "# check basic info for trump dataset to understand data types and missing values\n",
    "print(\"\\nTrump data info:\")\n",
    "print(df_trump.info())\n",
    "\n",
    "# check basic info for biden dataset to understand data types and missing values\n",
    "print(\"\\nBiden data info:\")\n",
    "print(df_biden.info())\n",
    "\n",
    "# calculate the distribution of tweet lengths in the trump data\n",
    "df_trump['tweet_length'] = df_trump['cleaned_tweet'].apply(len)  # add tweet length column for trump tweets\n",
    "print(\"\\nTrump tweet length statistics:\")\n",
    "print(df_trump['tweet_length'].describe())  # show summary statistics for tweet length\n",
    "\n",
    "# calculate the distribution of tweet lengths in the biden data\n",
    "df_biden['tweet_length'] = df_biden['cleaned_tweet'].apply(len)  # add tweet length column for biden tweets\n",
    "print(\"\\nBiden tweet length statistics:\")\n",
    "print(df_biden['tweet_length'].describe())  # show summary statistics for tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1748068, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# combine trump and biden tweets for vectorization\n",
    "tweets = pd.concat([df_trump['cleaned_tweet'], df_biden['cleaned_tweet']], ignore_index=True)  # merged tweet data\n",
    "\n",
    "# initialize tf-idf vectorizer w specified parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=10, stop_words='english', max_features=1000)  # configure tf-idf\n",
    "\n",
    "# fit tf-idf vectorizer and transform tweets to create tf-idf matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tweets)  # generate tf-idf matrix from tweet data\n",
    "\n",
    "# check shape of tf-idf matrix to confirm dimensions\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)  # print dimensions of tf-idf matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trump tweets with cluster labels:\n",
      "                                       cleaned_tweet  cluster\n",
      "0  elecciones2020  en florida joebiden dice que d...        4\n",
      "1  usa 2020 trump contro facebook e twitter copro...        0\n",
      "2  trump as a student i used to hear for years fo...        0\n",
      "3  2 hours since last tweet from trump maybe he i...        0\n",
      "4  you get a tie and you get a tie trump s rally ...        0\n",
      "\n",
      "Biden tweets with cluster labels:\n",
      "                                       cleaned_tweet  cluster\n",
      "0  elecciones2020  en florida joebiden dice que d...        4\n",
      "1  hunterbiden hunterbidenemails joebiden joebide...        3\n",
      "2     this is how biden made his  trumpisnotameri...        1\n",
      "3   watching and setting dvr lets give him bonus ...        3\n",
      "4  censorship hunterbiden biden bidenemails biden...        0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5  # set number of clusters for k-means\n",
    "\n",
    "# init k-means clustering w specified number of clusters and random state\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)  # init k-means w fixed clusters\n",
    "\n",
    "# fit k-means on the tf-idf matrix to find clusters in tweet data\n",
    "kmeans.fit(tfidf_matrix)  # train k-means clustering on tf-idf matrix\n",
    "\n",
    "# assign each tweet to a cluster based on fitted model\n",
    "tweets_clusters = kmeans.labels_  # array of cluster assignments for each tweet\n",
    "\n",
    "# add cluster labels back to trump and biden dataframes\n",
    "df_trump['cluster'] = tweets_clusters[:len(df_trump)]  # assign clusters to trump tweets\n",
    "df_biden['cluster'] = tweets_clusters[len(df_trump):]  # assign clusters to biden tweets\n",
    "\n",
    "# display cluster labels for a sample of trump tweets\n",
    "print(\"\\nTrump tweets with cluster labels:\")\n",
    "print(df_trump[['cleaned_tweet', 'cluster']].head())  # sample trump tweets w clusters\n",
    "\n",
    "# display cluster labels for a sample of biden tweets\n",
    "print(\"\\nBiden tweets with cluster labels:\")\n",
    "print(df_biden[['cleaned_tweet', 'cluster']].head())  # sample biden tweets w clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0:\n",
      "Top terms: ['trump', 'biden', 'election2020', 'donaldtrump', 'vote', 'amp', 'trump2020', 'elections2020', 'election', 'president']\n",
      "Sample tweets: ['usa 2020 trump contro facebook e twitter coprono biden\\xa0  donaldtrump  ', 'trump as a student i used to hear for years for ten years i heard china in 2019 and we have 15 and they dont know how many we have and i asked them how many do we have and they said sir we dont know but we have millions like 300 million  um what', '2 hours since last tweet from trump maybe he is very busy tremendously busy', 'censorship hunterbiden biden bidenemails bidenemail corruption ', 'in 2020 nypost is being censorship censored by twitter to manipulate a us election in favor of joebiden and against trump  but ccp from china or porn on twitter   thats always been fine for       is  sick', ' tell politicians to stick it with this free item    2020 biden deomocrat election politician politics president republican trump vpdebate   ']\n",
      "\n",
      "Cluster 1:\n",
      "Top terms: ['biden', 'trump', 'election2020', 'elections2020', 'vote', 'joe', 'donaldtrump', 'president', 'bidenharris2020', 'usa']\n",
      "Sample tweets: ['nbcnews wtf    cheduling its trump  ham townhall against abcnews legit biden  at 30rock studios   please disinfect snl   ', 'biden will delay the vaccine trump', ' biden trump', '   this is how biden made his  trumpisnotamerica  ', 'biden ', 'biden ']\n",
      "\n",
      "Cluster 2:\n",
      "Top terms: ['trump', 'biden', 'donald', 'trump2020', 'president', 'vote', 'election2020', 'elections2020', 'joebiden', 'donaldtrump']\n",
      "Sample tweets: ['trump ', ' demsarecorrupt trump fourmoreyears ', 'trump  sht  show  ', 'no ratings for trump catch the trump highlights afterwards biden trumpisachicken ', 'trump biden nckol email haber kstlama trump radikal sol hareketin 3 kolu gibi alyorlar  ', 'not even 7 mins in and trump brings up northam biden trump townhalls']\n",
      "\n",
      "Cluster 3:\n",
      "Top terms: ['joebiden', 'donaldtrump', 'kamalaharris', 'election2020', 'president', 'joe', 'vote', 'bidenharris2020', 'biden', 'america']\n",
      "Sample tweets: ['is this wrong cory bookers brilliant final questioning of trump nominee amy coney barrett  amyconeybarrett corybooker barrett booker trump kamalaharris joebiden scotus supremecourtconfirmation', 'a simple question who are you voting for 2020election donaldtrump joebiden uspoli mapoli', 'biden for president trump four more years  donaldtrump joebiden election2020 ', 'hunterbiden hunterbidenemails joebiden joebidenmuststepdown ', ' watching and setting dvr lets give him bonus ratings joebiden', 'is this wrong cory bookers brilliant final questioning of trump nominee amy coney barrett  amyconeybarrett corybooker barrett booker trump kamalaharris joebiden scotus supremecourtconfirmation']\n",
      "\n",
      "Cluster 4:\n",
      "Top terms: ['la', 'que', 'en', 'el', 'trump', 'le', 'biden', 'los', 'se', 'les']\n",
      "Sample tweets: ['elecciones2020  en florida joebiden dice que donaldtrump solo se preocupa por l mismo el demcrata fue anfitrin de encuentros de electores en pembrokepines y miramar clic aqu     elsollatino yobrilloconelsol ', '     otro jonrn de trump y que no se nos olvide que ah tambin estn involucrados los clintoncrimefamily y obama por mencionar algunos ', 'las campaas de biden en la comunidad latina literal es nomas el diciendo hola no soy trump vota por mi ', 'elecciones2020  en florida joebiden dice que donaldtrump solo se preocupa por l mismo el demcrata fue anfitrin de encuentros de electores en pembrokepines y miramar clic aqu     elsollatino yobrilloconelsol ', 'las campaas de biden en la comunidad latina literal es nomas el diciendo hola no soy trump vota por mi ', 'la red social twitter restringi el acceso a sus usuarios a un vnculo publicado en un tuit que diriga a una pgina web del diario  donde se mostraban los mensajes de correo electrnico entre un empresario ucraniano y hunterbiden hijo de joebiden ']\n"
     ]
    }
   ],
   "source": [
    "# get feature names (terms) from tf-idf vectorizer for understanding clusters\n",
    "terms = tfidf_vectorizer.get_feature_names_out() # list of terms used in tf-idf matrix\n",
    "\n",
    "# get top terms for each cluster center, ordered by importance\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1] # get indices of top terms per cluster\n",
    "\n",
    "print(\"Top terms per cluster:\") # start output for cluster analysis\n",
    "# loop through each cluster to display top terms and sample tweets\n",
    "for i in range(num_clusters):\n",
    "    print(f\"\\nCluster {i}:\") # print cluster number\n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]] # get top 10 terms for current cluster\n",
    "    print(\"Top terms:\", top_terms) # print top terms for current cluster\n",
    "\n",
    "    # display sample tweets from trump and biden in current cluster\n",
    "    sample_tweets = df_trump[df_trump['cluster'] == i]['cleaned_tweet'].head(3).tolist() # trump sample tweets\n",
    "    sample_tweets += df_biden[df_biden['cluster'] == i]['cleaned_tweet'].head(3).tolist() # biden sample tweets\n",
    "    print(\"Sample tweets:\", sample_tweets) # print sample tweets for current cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentiment by Cluster for Trump Tweets:\n",
      "cluster\n",
      "0    0.046928\n",
      "1    0.006283\n",
      "2    0.000162\n",
      "3    0.067517\n",
      "4    0.005565\n",
      "Name: sentiment_score, dtype: float64\n",
      "\n",
      "Average Sentiment by Cluster for Biden Tweets:\n",
      "cluster\n",
      "0    0.073713\n",
      "1    0.007577\n",
      "2    0.005295\n",
      "3    0.082120\n",
      "4    0.008179\n",
      "Name: sentiment_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# function to get sentiment score for each tweet\n",
    "def get_sentiment_score(text):  # returns polarity of the text (-1 to 1)\n",
    "    analysis = TextBlob(text)  # create TextBlob object\n",
    "    return analysis.sentiment.polarity  # return polarity score\n",
    "\n",
    "# apply sentiment analysis to each dataset\n",
    "df_trump['sentiment_score'] = df_trump['cleaned_tweet'].apply(get_sentiment_score)  # trump sentiment\n",
    "df_biden['sentiment_score'] = df_biden['cleaned_tweet'].apply(get_sentiment_score)  # biden sentiment\n",
    "\n",
    "# calculate average sentiment for each cluster\n",
    "trump_cluster_sentiment = df_trump.groupby('cluster')['sentiment_score'].mean()  # trump avg sentiment by cluster\n",
    "biden_cluster_sentiment = df_biden.groupby('cluster')['sentiment_score'].mean()  # biden avg sentiment by cluster\n",
    "\n",
    "# print sentiment results\n",
    "print(\"Average Sentiment by Cluster for Trump Tweets:\")\n",
    "print(trump_cluster_sentiment)\n",
    "\n",
    "print(\"\\nAverage Sentiment by Cluster for Biden Tweets:\")\n",
    "print(biden_cluster_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 Top Terms:\n",
      "Top terms: ['trump', 'biden', 'election2020', 'donaldtrump', 'vote', 'amp', 'trump2020', 'elections2020', 'election', 'president']\n",
      "\n",
      "Cluster 1 Top Terms:\n",
      "Top terms: ['biden', 'trump', 'election2020', 'elections2020', 'vote', 'joe', 'donaldtrump', 'president', 'bidenharris2020', 'usa']\n",
      "\n",
      "Cluster 2 Top Terms:\n",
      "Top terms: ['trump', 'biden', 'donald', 'trump2020', 'president', 'vote', 'election2020', 'elections2020', 'joebiden', 'donaldtrump']\n",
      "\n",
      "Cluster 3 Top Terms:\n",
      "Top terms: ['joebiden', 'donaldtrump', 'kamalaharris', 'election2020', 'president', 'joe', 'vote', 'bidenharris2020', 'biden', 'america']\n",
      "\n",
      "Cluster 4 Top Terms:\n",
      "Top terms: ['la', 'que', 'en', 'el', 'trump', 'le', 'biden', 'los', 'se', 'les']\n"
     ]
    }
   ],
   "source": [
    "# get feature names from tf-idf vectorizer\n",
    "terms = tfidf_vectorizer.get_feature_names_out()  # list of terms in tf-idf matrix\n",
    "\n",
    "# get top terms for each cluster center\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]  # indices of terms ordered by importance for each cluster\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "# loop through each cluster to print top terms\n",
    "for i in range(num_clusters):\n",
    "    print(f\"\\nCluster {i} Top Terms:\")  # print cluster number\n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # get top 10 terms for current cluster\n",
    "    print(\"Top terms:\", top_terms)  # print top terms for current cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geospatial analysis\n",
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "def get_coordinates(location):\n",
    "    if pd.isnull(location) or location.strip() == '':\n",
    "        return None\n",
    "    try:\n",
    "        loc = geolocator.geocode(location)\n",
    "        return [loc.latitude, loc.longitude] if loc else None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "df_trump['coordinates'] = df_trump['state'].apply(get_coordinates)\n",
    "df_biden['coordinates'] = df_biden['state'].apply(get_coordinates)\n",
    "\n",
    "# Create the heatmap visualization\n",
    "def create_heatmap(data, title):\n",
    "    map_ = folium.Map(location=[37.0902, -95.7129], zoom_start=5)\n",
    "    heat_data = [coords for coords in data['coordinates'] if coords]  # remove invalid entries\n",
    "    HeatMap(heat_data).add_to(map_)\n",
    "    map_.save(f\"{title}.html\")\n",
    "    print(f\"Heatmap saved as {title}.html\")\n",
    "\n",
    "create_heatmap(df_trump, \"trump_sentiment_heatmap\")\n",
    "create_heatmap(df_biden, \"biden_sentiment_heatmap\")\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "def plot_sentiment(data, title):\n",
    "    sns.countplot(x='sentiment', data=data, palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment(df_trump, \"Trump Tweet Sentiments\")\n",
    "plot_sentiment(df_biden, \"Biden Tweet Sentiments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
